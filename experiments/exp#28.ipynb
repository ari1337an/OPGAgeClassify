{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "WpCAGQa5_yxX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WpCAGQa5_yxX",
    "outputId": "2a3afaca-1829-44f6-8470-aee27df92cbb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: pytorch-metric-learning in /usr/local/lib/python3.10/dist-packages (2.1.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pytorch-metric-learning) (1.22.4)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from pytorch-metric-learning) (1.2.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pytorch-metric-learning) (4.65.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-metric-learning) (2.0.1+cu118)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (4.5.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (1.11.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (3.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (2.0.0)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->pytorch-metric-learning) (3.25.2)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->pytorch-metric-learning) (16.0.5)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pytorch-metric-learning) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pytorch-metric-learning) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pytorch-metric-learning) (3.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->pytorch-metric-learning) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->pytorch-metric-learning) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-metric-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdHENHQJkaNa",
   "metadata": {
    "id": "bdHENHQJkaNa"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import zipfile\n",
    "import subprocess\n",
    "import shutil\n",
    "import getpass\n",
    "import math\n",
    "import numpy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import read_image,ImageReadMode\n",
    "import matplotlib.pyplot as plt\n",
    "from pytorch_metric_learning import losses, regularizers\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6C9vAiat-6F",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d6C9vAiat-6F",
    "outputId": "dcfda69e-b5ea-4d8f-c436-c4ddd02ee226"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fd2ec0b90f0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "OxcfShJ_uDVs",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OxcfShJ_uDVs",
    "outputId": "a8d2dca1-20a5-4f29-84d8-46334f8d7c5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ZTOkACl6d2s5",
   "metadata": {
    "id": "ZTOkACl6d2s5"
   },
   "outputs": [],
   "source": [
    "dataset_save_dir = './dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdA042w3vQUk",
   "metadata": {
    "id": "fdA042w3vQUk"
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(val):\n",
    "    arr = numpy.zeros((6,), dtype=int)\n",
    "    arr[val] = 1\n",
    "    return arr\n",
    "\n",
    "def get_bucket_id(age):\n",
    "  age_floor = int(age)\n",
    "  if age_floor >= 0 and age_floor <= 5: return 0\n",
    "  elif age_floor >= 6 and age_floor <= 12: return 1\n",
    "  elif age_floor >= 13 and age_floor <= 19: return 2\n",
    "  elif age_floor >= 20 and age_floor <= 29: return 3\n",
    "  elif age_floor >= 30 and age_floor <= 59: return 4\n",
    "  else: return 5\n",
    "\n",
    "def get_ground_truth(age):\n",
    "  return one_hot_encode(get_bucket_id(age))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "VbbhYSphI1C_",
   "metadata": {
    "id": "VbbhYSphI1C_"
   },
   "outputs": [],
   "source": [
    "def get_random_two_different_int(low=0, high=6, size=1):\n",
    "  num1 = torch.randint(low,high, (size,)).item()\n",
    "  num2 = torch.randint(low,high, (size,)).item()\n",
    "  while num1 == num2: num2 = torch.randint(low,high, (size,)).item()\n",
    "  return num1,num2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "uVNdhyjdwbMM",
   "metadata": {
    "id": "uVNdhyjdwbMM"
   },
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    torch.cuda.empty_cache()\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    loss_tot = 0.0\n",
    "    num = 0\n",
    "    for batch, (X1, y1, X2, y2) in enumerate(dataloader):\n",
    "        X1, y1, X2, y2 = X1.to(device), y1, X2.to(device), y2\n",
    "        targets = torch.eq(y1.argmax(dim=1), y2.argmax(dim=1)).to(torch.float32).to(device)\n",
    "\n",
    "        # Forward\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X1, X2)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_tot += loss.item()\n",
    "        num += 1\n",
    "        \n",
    "        X1.cpu()\n",
    "        X2.cpu()\n",
    "        targets.cpu()\n",
    "\n",
    "        # Gather data and report\n",
    "        if batch % 4 == 0:\n",
    "            current = (batch + 1) * len(X1)\n",
    "            print(f\"loss: {loss.item():>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    # loss_tot /= num\n",
    "    print(f'training loss: {(loss_tot):>0.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "QSXDRsiqolDJ",
   "metadata": {
    "id": "QSXDRsiqolDJ"
   },
   "outputs": [],
   "source": [
    "validation_accuracy = []\n",
    "current_max_val_acc = 0.0\n",
    "def validation(dataset, model, loss_fn):\n",
    "    model.eval()\n",
    "    global current_max_val_acc\n",
    "    size = len(dataset)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss_tot = 0\n",
    "    with torch.no_grad():\n",
    "        for i in range(size):\n",
    "            XQ, yQ = dataset[i]\n",
    "            XQ, yQ = XQ.reshape(-1,3,224,224).to(device), yQ.to(device)\n",
    "            best = [-1000,-1000,-1000,-1000,-1000,-1000]\n",
    "            rem = [1,1,1,1,1,1]\n",
    "            anchors = []\n",
    "            for j in range(size):\n",
    "                if i == j: continue\n",
    "                classLabel = (dataset[j][1]).argmax(dim=0)\n",
    "                if rem[classLabel] <= 0: continue\n",
    "                anchors.append(j)\n",
    "                rem[classLabel] = rem[classLabel] - 1\n",
    "                if sum(rem) <= 0: break \n",
    "\n",
    "            for j in anchors:\n",
    "                XR, yR = dataset[j]\n",
    "                classLabel = yR.argmax(dim=0)\n",
    "                XR, yR = XR.reshape(-1,3,224,224).to(device), yR.to(device)\n",
    "                rem[classLabel] = rem[classLabel] - 1\n",
    "                output = model(XQ,XR)\n",
    "                targets = torch.eq(yQ.argmax(dim=0), yR.argmax(dim=0)).to(torch.float32).reshape(-1).to(device)\n",
    "                loss = loss_fn(output, targets)\n",
    "                best[classLabel] = max(best[classLabel], output.cpu().numpy())\n",
    "                loss_tot += loss.item() \n",
    "            classified_as = numpy.argmax(best)\n",
    "            correct += (classified_as == yQ.argmax(dim=0).cpu())\n",
    "            total = total + 1\n",
    "            \n",
    "    print(f\"Correct/Total: {correct}/{total}\")\n",
    "    correct = correct*1.0 / total\n",
    "    validation_accuracy.append(correct*100)\n",
    "    print(f\"Validation Accuracy: {(100*correct):>0.5f}%\\n\")\n",
    "    current_max_val_acc = max(current_max_val_acc,100*correct)\n",
    "    print(f\"Current Best Validation Accuracy: {(current_max_val_acc):>0.5f}%\\n\")\n",
    "    return loss_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "AHwUHW0XvX0s",
   "metadata": {
    "id": "AHwUHW0XvX0s"
   },
   "outputs": [],
   "source": [
    "data_augmentation_transformations = T.RandomChoice([ # Geometric Transformation\n",
    "    T.RandomAffine(degrees=0),\n",
    "    T.Lambda(lambda x: TF.hflip(img=x))\n",
    "    # T.RandomAffine(degrees=0), # No Transformation\n",
    "    # Geometric Transformations:\n",
    "    # T.RandomAffine(degrees=0, scale=(1.3,1.3)), # Scale\n",
    "    # T.RandomAffine(degrees=0, translate=(0.5,0.5)), # Translate\n",
    "    # T.RandomAffine(degrees=(-8, 8)), # Rotate\n",
    "    # T.Lambda(lambda x: TF.hflip(img=x)), # Reflect\n",
    "    # Skipping Shearing & Skewing as they don't make sense in this context of Teeth X-Ray\n",
    "    # Occlusion:\n",
    "    # T.Compose([T.RandomErasing(p=1, scale=(0.0008, 0.0008), ratio=(1,1))]*100), # Occlusion\n",
    "    # T.Compose([T.RandomErasing(p=1, scale=(0.0008, 0.0008), ratio=(1,1))]*100), # Occlusion\n",
    "    # T.Compose([T.RandomErasing(p=1, scale=(0.0008, 0.0008), ratio=(1,1))]*100), # Occlusion\n",
    "    # Intensity Operations\n",
    "    # T.Lambda(lambda x: TF.adjust_gamma(img=x, gamma=0.5)), # Gamma Contrast\n",
    "    # T.Lambda(lambda x: TF.adjust_contrast(x, contrast_factor=2.0)), # Linear Contrast\n",
    "    # Histogram Equalizer skipped as we need to typecast it to uint8 for that\n",
    "    # Skipping Noise injection as we want to easily normalize it later \n",
    "    # Filtering:\n",
    "    # T.Lambda(lambda x: TF.adjust_sharpness(img=x, sharpness_factor=4)), #Sharpen\n",
    "    # T.GaussianBlur(kernel_size=(15,15), sigma=(0.01, 1)), # Gaussian Blur\n",
    "])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "QXdT7TGwECqD",
   "metadata": {
    "id": "QXdT7TGwECqD"
   },
   "outputs": [],
   "source": [
    "class XRayToothDatasetPosNeg(Dataset):\n",
    "    def __init__(self, cwd, img_dir, transform=None, target_height=None, target_width=None):\n",
    "        self.dataset_path = cwd + '/' + img_dir\n",
    "        self.transform = transform\n",
    "        self.target_height = target_height\n",
    "        self.target_width = target_width\n",
    "\n",
    "        # Group the examples based on their label\n",
    "        self.grouped_examples = {}\n",
    "        for filename in os.listdir(self.dataset_path):\n",
    "            age_bracket = get_bucket_id(float(filename.split(\"_\")[1][:-4]))\n",
    "            if age_bracket not in self.grouped_examples: self.grouped_examples[age_bracket] = []\n",
    "            self.grouped_examples[age_bracket].append(filename)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(os.listdir(self.dataset_path))\n",
    "\n",
    "    def get_datasample(self, img_filename):\n",
    "        age = float(img_filename.split(\"_\")[1][:-4])\n",
    "        age_gt = get_ground_truth(age)\n",
    "        image_tensor = read_image(path=self.dataset_path + '/' + img_filename)\n",
    "        image_tensor = image_tensor.reshape(1, 3, image_tensor.shape[-2], image_tensor.shape[-1])\n",
    "        if self.target_height and self.target_width: # Resize the image \n",
    "            image_tensor = torch.nn.functional.interpolate(image_tensor, (self.target_height,self.target_width))\n",
    "        if self.transform: image_tensor = self.transform(image_tensor) # Apply transformations\n",
    "        image_tensor = (image_tensor-image_tensor.min())/(image_tensor.max()-image_tensor.min())\n",
    "        return image_tensor.reshape(-1,image_tensor.shape[-2],image_tensor.shape[-1]).to(torch.float32), torch.tensor(age_gt)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx  >= len(os.listdir(self.dataset_path)):\n",
    "            print(\"No datafile/image at index : \"+ str(idx))\n",
    "            return None\n",
    "\n",
    "        if idx%2 == 0: # Give them a positive example\n",
    "            selected_class = torch.randint(0,6,(1,)).item()\n",
    "            index1,index2 = get_random_two_different_int(0,len(self.grouped_examples[selected_class]),1)\n",
    "            image1, label1 = self.get_datasample(self.grouped_examples[selected_class][index1])\n",
    "            image2, label2 = self.get_datasample(self.grouped_examples[selected_class][index2])\n",
    "            return (image1,label1,image2,label2)\n",
    "        else:\n",
    "            # Give them a negative example\n",
    "            selected_class1, selected_class2 = get_random_two_different_int(0,6,1)\n",
    "            index1 = torch.randint(0,len(self.grouped_examples[selected_class1]),(1,)).item()\n",
    "            index2 = torch.randint(0,len(self.grouped_examples[selected_class2]),(1,)).item()\n",
    "            image1, label1 = self.get_datasample(self.grouped_examples[selected_class1][index1])\n",
    "            image2, label2 = self.get_datasample(self.grouped_examples[selected_class2][index2])\n",
    "            return (image1,label1,image2,label2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "kTyfSsFwoNqO",
   "metadata": {
    "id": "kTyfSsFwoNqO"
   },
   "outputs": [],
   "source": [
    "class XRayToothDataset(Dataset):\n",
    "    def __init__(self, cwd, img_dir, transform=None, target_height=None, target_width=None):\n",
    "        self.dataset_path = cwd + '/' + img_dir\n",
    "        self.transform = transform\n",
    "        self.target_height = target_height\n",
    "        self.target_width = target_width\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(os.listdir(self.dataset_path))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx  >= len(os.listdir(self.dataset_path)):\n",
    "            print(\"No datafile/image at index : \"+ str(idx))\n",
    "            return None\n",
    "        img_filename = os.listdir(self.dataset_path)[idx]\n",
    "        age = float(img_filename.split(\"_\")[1][:-4])\n",
    "        age_gt = get_ground_truth(age)\n",
    "        image_tensor = read_image(path=self.dataset_path + '/' + img_filename)\n",
    "        image_tensor = image_tensor.reshape(1, 3, image_tensor.shape[-2], image_tensor.shape[-1])\n",
    "        if self.target_height and self.target_width: # Resize the image \n",
    "            image_tensor = torch.nn.functional.interpolate(image_tensor, (self.target_height,self.target_width))\n",
    "        if self.transform: image_tensor = self.transform(image_tensor) # Apply transformations\n",
    "        image_tensor = (image_tensor-image_tensor.min())/(image_tensor.max()-image_tensor.min())\n",
    "        return image_tensor.reshape(-1,image_tensor.shape[-2],image_tensor.shape[-1]).to(torch.float32), torch.tensor(age_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "GcLfF0ubvjzD",
   "metadata": {
    "id": "GcLfF0ubvjzD"
   },
   "outputs": [],
   "source": [
    "training_data = XRayToothDatasetPosNeg(os.getcwd(), img_dir=dataset_save_dir+'/training', transform=data_augmentation_transformations, target_height=224, target_width=224)\n",
    "validation_data = XRayToothDataset(os.getcwd(), img_dir=dataset_save_dir+'/validation', transform=None, target_height=224, target_width=224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "xV_5gqVqbGbF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xV_5gqVqbGbF",
    "outputId": "2d316ccf-3299-4a01-b49b-fc5573f0c2a3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vit_l_32-c7638314.pth\" to /root/.cache/torch/hub/checkpoints/vit_l_32-c7638314.pth\n",
      "100%|██████████| 1.14G/1.14G [00:13<00:00, 89.4MB/s]\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import vit_l_32, ViT_L_32_Weights\n",
    "\n",
    "pretrained_vit = vit_l_32(weights=ViT_L_32_Weights.IMAGENET1K_V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7_GafBd2vsM0",
   "metadata": {
    "id": "7_GafBd2vsM0"
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = pretrained_vit\n",
    "\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.6),\n",
    "            nn.Linear(1000,6)\n",
    "        )\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, xR, xQ):\n",
    "        e1 = self.fc(self.backbone(xR))\n",
    "        e2 = self.fc(self.backbone(xQ))\n",
    "\n",
    "        similarity = torch.linalg.vecdot(e1, e2)\n",
    "        output = self.sigmoid(similarity)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "895zQ65zv-oO",
   "metadata": {
    "id": "895zQ65zv-oO"
   },
   "outputs": [],
   "source": [
    "model = NeuralNetwork().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ZzpO5XU0wAmA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZzpO5XU0wAmA",
    "outputId": "976cc3f8-4d58-42ca-bfd0-bf368b3b5561"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3061], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Test a forward pass\n",
    "image1,label1,image2,label2 = training_data[0]\n",
    "with torch.no_grad():\n",
    "    print(model(image1.reshape(-1,3,224,224).to(device), image2.reshape(-1,3,224,224).to(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "Xy5dsAcpwCri",
   "metadata": {
    "id": "Xy5dsAcpwCri"
   },
   "outputs": [],
   "source": [
    "# Training Hyperparameters\n",
    "epochs = 500\n",
    "batch_size = 10\n",
    "learning_rate = 1e-4\n",
    "momentum=0.9\n",
    "weight_decay=0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "WhovjcHqwX91",
   "metadata": {
    "id": "WhovjcHqwX91"
   },
   "outputs": [],
   "source": [
    "training_data_loader = DataLoader(training_data, batch_size, shuffle = True)\n",
    "validation_data_loader = DataLoader(validation_data, batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "YenD-SOUwRAV",
   "metadata": {
    "id": "YenD-SOUwRAV"
   },
   "outputs": [],
   "source": [
    "loss_function=nn.BCELoss()\n",
    "optimizer=torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.9, patience=5, min_lr=1e-4,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s7NVlZBIwh-F",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s7NVlZBIwh-F",
    "outputId": "fe1dcdb4-39c6-4527-e959-6ee390514b54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.335649  [   10/  296]\n",
      "loss: 1.563321  [   50/  296]\n",
      "loss: 1.619494  [   90/  296]\n",
      "loss: 0.663186  [  130/  296]\n",
      "loss: 1.432854  [  170/  296]\n",
      "loss: 0.483766  [  210/  296]\n",
      "loss: 0.612344  [  250/  296]\n",
      "loss: 0.877295  [  290/  296]\n",
      "training loss: 29.29094\n",
      "Correct/Total: 16/129\n",
      "Validation Accuracy: 12.40310%\n",
      "\n",
      "Current Best Validation Accuracy: 12.40310%\n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.783023  [   10/  296]\n",
      "loss: 0.423807  [   50/  296]\n",
      "loss: 0.700526  [   90/  296]\n",
      "loss: 0.942276  [  130/  296]\n",
      "loss: 0.624152  [  170/  296]\n",
      "loss: 1.184766  [  210/  296]\n",
      "loss: 1.323326  [  250/  296]\n",
      "loss: 0.501082  [  290/  296]\n",
      "training loss: 28.05611\n",
      "Correct/Total: 15/129\n",
      "Validation Accuracy: 11.62791%\n",
      "\n",
      "Current Best Validation Accuracy: 12.40310%\n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.720139  [   10/  296]\n",
      "loss: 0.952585  [   50/  296]\n",
      "loss: 1.073041  [   90/  296]\n",
      "loss: 0.411474  [  130/  296]\n",
      "loss: 0.515103  [  170/  296]\n",
      "loss: 0.683587  [  210/  296]\n",
      "loss: 1.081126  [  250/  296]\n",
      "loss: 0.813528  [  290/  296]\n",
      "training loss: 26.97810\n",
      "Correct/Total: 21/129\n",
      "Validation Accuracy: 16.27907%\n",
      "\n",
      "Current Best Validation Accuracy: 16.27907%\n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.908476  [   10/  296]\n",
      "loss: 0.996907  [   50/  296]\n",
      "loss: 1.400168  [   90/  296]\n",
      "loss: 0.816586  [  130/  296]\n",
      "loss: 0.666428  [  170/  296]\n",
      "loss: 1.295357  [  210/  296]\n",
      "loss: 0.846316  [  250/  296]\n",
      "loss: 1.089684  [  290/  296]\n",
      "training loss: 26.70515\n",
      "Correct/Total: 27/129\n",
      "Validation Accuracy: 20.93023%\n",
      "\n",
      "Current Best Validation Accuracy: 20.93023%\n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.707488  [   10/  296]\n",
      "loss: 0.759523  [   50/  296]\n",
      "loss: 0.795186  [   90/  296]\n",
      "loss: 1.082639  [  130/  296]\n",
      "loss: 0.757029  [  170/  296]\n",
      "loss: 0.928621  [  210/  296]\n",
      "loss: 0.730156  [  250/  296]\n",
      "loss: 0.984671  [  290/  296]\n",
      "training loss: 25.68862\n",
      "Correct/Total: 28/129\n",
      "Validation Accuracy: 21.70543%\n",
      "\n",
      "Current Best Validation Accuracy: 21.70543%\n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.630470  [   10/  296]\n",
      "loss: 1.200373  [   50/  296]\n",
      "loss: 0.886088  [   90/  296]\n",
      "loss: 0.876652  [  130/  296]\n",
      "loss: 0.961922  [  170/  296]\n",
      "loss: 0.923152  [  210/  296]\n",
      "loss: 0.599121  [  250/  296]\n",
      "loss: 1.038155  [  290/  296]\n",
      "training loss: 28.52957\n",
      "Correct/Total: 26/129\n",
      "Validation Accuracy: 20.15504%\n",
      "\n",
      "Current Best Validation Accuracy: 21.70543%\n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.417128  [   10/  296]\n",
      "loss: 0.576547  [   50/  296]\n",
      "loss: 0.730791  [   90/  296]\n",
      "loss: 0.588278  [  130/  296]\n",
      "loss: 0.640170  [  170/  296]\n",
      "loss: 1.076176  [  210/  296]\n",
      "loss: 0.660313  [  250/  296]\n",
      "loss: 0.643131  [  290/  296]\n",
      "training loss: 28.78208\n",
      "Correct/Total: 26/129\n",
      "Validation Accuracy: 20.15504%\n",
      "\n",
      "Current Best Validation Accuracy: 21.70543%\n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1.029145  [   10/  296]\n",
      "loss: 0.713478  [   50/  296]\n",
      "loss: 1.248450  [   90/  296]\n",
      "loss: 1.253589  [  130/  296]\n",
      "loss: 0.812248  [  170/  296]\n",
      "loss: 0.644794  [  210/  296]\n",
      "loss: 0.905393  [  250/  296]\n",
      "loss: 1.168347  [  290/  296]\n",
      "training loss: 27.93658\n",
      "Correct/Total: 20/129\n",
      "Validation Accuracy: 15.50388%\n",
      "\n",
      "Current Best Validation Accuracy: 21.70543%\n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.573511  [   10/  296]\n",
      "loss: 1.329607  [   50/  296]\n",
      "loss: 0.835561  [   90/  296]\n",
      "loss: 1.538210  [  130/  296]\n",
      "loss: 0.766513  [  170/  296]\n",
      "loss: 0.931504  [  210/  296]\n",
      "loss: 0.402871  [  250/  296]\n",
      "loss: 0.663440  [  290/  296]\n",
      "training loss: 26.90763\n",
      "Correct/Total: 19/129\n",
      "Validation Accuracy: 14.72868%\n",
      "\n",
      "Current Best Validation Accuracy: 21.70543%\n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.807636  [   10/  296]\n",
      "loss: 1.077799  [   50/  296]\n",
      "loss: 0.640887  [   90/  296]\n",
      "loss: 1.056989  [  130/  296]\n",
      "loss: 1.030048  [  170/  296]\n",
      "loss: 1.144408  [  210/  296]\n",
      "loss: 1.000740  [  250/  296]\n",
      "loss: 0.892005  [  290/  296]\n",
      "training loss: 27.00327\n",
      "Correct/Total: 39/129\n",
      "Validation Accuracy: 30.23256%\n",
      "\n",
      "Current Best Validation Accuracy: 30.23256%\n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.468858  [   10/  296]\n",
      "loss: 0.782854  [   50/  296]\n",
      "loss: 1.357970  [   90/  296]\n",
      "loss: 0.761228  [  130/  296]\n",
      "loss: 0.354549  [  170/  296]\n",
      "loss: 1.038131  [  210/  296]\n",
      "loss: 0.762222  [  250/  296]\n",
      "loss: 1.054733  [  290/  296]\n",
      "training loss: 25.40932\n",
      "Correct/Total: 48/129\n",
      "Validation Accuracy: 37.20930%\n",
      "\n",
      "Current Best Validation Accuracy: 37.20930%\n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 1.156226  [   10/  296]\n",
      "loss: 0.526087  [   50/  296]\n",
      "loss: 0.546473  [   90/  296]\n",
      "loss: 1.077345  [  130/  296]\n",
      "loss: 1.175847  [  170/  296]\n",
      "loss: 0.761755  [  210/  296]\n",
      "loss: 1.101355  [  250/  296]\n",
      "loss: 0.812105  [  290/  296]\n",
      "training loss: 26.85067\n",
      "Correct/Total: 29/129\n",
      "Validation Accuracy: 22.48062%\n",
      "\n",
      "Current Best Validation Accuracy: 37.20930%\n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.807800  [   10/  296]\n",
      "loss: 0.848611  [   50/  296]\n",
      "loss: 1.432157  [   90/  296]\n",
      "loss: 0.649971  [  130/  296]\n",
      "loss: 0.555833  [  170/  296]\n",
      "loss: 0.622238  [  210/  296]\n",
      "loss: 1.306414  [  250/  296]\n",
      "loss: 0.730975  [  290/  296]\n",
      "training loss: 25.37231\n",
      "Correct/Total: 27/129\n",
      "Validation Accuracy: 20.93023%\n",
      "\n",
      "Current Best Validation Accuracy: 37.20930%\n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.776223  [   10/  296]\n",
      "loss: 1.171235  [   50/  296]\n",
      "loss: 0.846717  [   90/  296]\n",
      "loss: 0.779681  [  130/  296]\n",
      "loss: 0.567565  [  170/  296]\n",
      "loss: 0.860539  [  210/  296]\n",
      "loss: 0.686184  [  250/  296]\n",
      "loss: 0.922014  [  290/  296]\n",
      "training loss: 23.90914\n",
      "Correct/Total: 31/129\n",
      "Validation Accuracy: 24.03101%\n",
      "\n",
      "Current Best Validation Accuracy: 37.20930%\n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.791189  [   10/  296]\n",
      "loss: 0.645734  [   50/  296]\n",
      "loss: 0.656774  [   90/  296]\n",
      "loss: 0.464850  [  130/  296]\n",
      "loss: 1.090189  [  170/  296]\n",
      "loss: 1.008953  [  210/  296]\n",
      "loss: 0.794566  [  250/  296]\n",
      "loss: 0.500569  [  290/  296]\n",
      "training loss: 23.09209\n",
      "Correct/Total: 43/129\n",
      "Validation Accuracy: 33.33334%\n",
      "\n",
      "Current Best Validation Accuracy: 37.20930%\n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 1.102043  [   10/  296]\n",
      "loss: 1.075523  [   50/  296]\n",
      "loss: 0.776702  [   90/  296]\n",
      "loss: 1.620813  [  130/  296]\n",
      "loss: 0.900431  [  170/  296]\n",
      "loss: 0.434605  [  210/  296]\n",
      "loss: 0.996083  [  250/  296]\n",
      "loss: 0.614731  [  290/  296]\n",
      "training loss: 25.59502\n",
      "Correct/Total: 32/129\n",
      "Validation Accuracy: 24.80620%\n",
      "\n",
      "Current Best Validation Accuracy: 37.20930%\n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 1.146961  [   10/  296]\n",
      "loss: 0.660897  [   50/  296]\n",
      "loss: 0.871350  [   90/  296]\n",
      "loss: 0.881925  [  130/  296]\n",
      "loss: 1.723425  [  170/  296]\n",
      "loss: 0.800396  [  210/  296]\n",
      "loss: 0.757629  [  250/  296]\n",
      "loss: 0.886782  [  290/  296]\n",
      "training loss: 27.26581\n",
      "Correct/Total: 48/129\n",
      "Validation Accuracy: 37.20930%\n",
      "\n",
      "Current Best Validation Accuracy: 37.20930%\n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.968802  [   10/  296]\n",
      "loss: 0.888072  [   50/  296]\n",
      "loss: 0.834365  [   90/  296]\n",
      "loss: 0.616602  [  130/  296]\n",
      "loss: 1.128700  [  170/  296]\n",
      "loss: 1.004613  [  210/  296]\n",
      "loss: 0.780965  [  250/  296]\n",
      "loss: 0.993525  [  290/  296]\n",
      "training loss: 24.95899\n",
      "Correct/Total: 24/129\n",
      "Validation Accuracy: 18.60465%\n",
      "\n",
      "Current Best Validation Accuracy: 37.20930%\n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 1.266858  [   10/  296]\n",
      "loss: 0.461813  [   50/  296]\n",
      "loss: 1.010086  [   90/  296]\n",
      "loss: 0.737645  [  130/  296]\n",
      "loss: 1.083808  [  170/  296]\n",
      "loss: 1.796202  [  210/  296]\n",
      "loss: 0.952460  [  250/  296]\n",
      "loss: 0.765814  [  290/  296]\n",
      "training loss: 27.48676\n",
      "Correct/Total: 40/129\n",
      "Validation Accuracy: 31.00775%\n",
      "\n",
      "Current Best Validation Accuracy: 37.20930%\n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.752417  [   10/  296]\n",
      "loss: 0.563983  [   50/  296]\n",
      "loss: 0.799725  [   90/  296]\n",
      "loss: 0.853269  [  130/  296]\n",
      "loss: 0.882344  [  170/  296]\n",
      "loss: 0.740864  [  210/  296]\n",
      "loss: 0.671399  [  250/  296]\n",
      "loss: 0.896176  [  290/  296]\n",
      "training loss: 24.34482\n",
      "Correct/Total: 46/129\n",
      "Validation Accuracy: 35.65891%\n",
      "\n",
      "Current Best Validation Accuracy: 37.20930%\n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 1.261900  [   10/  296]\n",
      "loss: 1.177566  [   50/  296]\n",
      "loss: 0.827000  [   90/  296]\n",
      "loss: 0.443518  [  130/  296]\n",
      "loss: 0.355670  [  170/  296]\n",
      "loss: 0.921844  [  210/  296]\n",
      "loss: 1.193631  [  250/  296]\n",
      "loss: 0.693388  [  290/  296]\n",
      "training loss: 23.82978\n",
      "Correct/Total: 67/129\n",
      "Validation Accuracy: 51.93798%\n",
      "\n",
      "Current Best Validation Accuracy: 51.93798%\n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.584759  [   10/  296]\n",
      "loss: 1.024394  [   50/  296]\n",
      "loss: 0.985209  [   90/  296]\n",
      "loss: 0.928263  [  130/  296]\n",
      "loss: 0.384312  [  170/  296]\n",
      "loss: 0.687008  [  210/  296]\n",
      "loss: 0.949545  [  250/  296]\n",
      "loss: 0.551783  [  290/  296]\n",
      "training loss: 23.62391\n",
      "Correct/Total: 65/129\n",
      "Validation Accuracy: 50.38760%\n",
      "\n",
      "Current Best Validation Accuracy: 51.93798%\n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 1.402345  [   10/  296]\n",
      "loss: 0.421573  [   50/  296]\n",
      "loss: 0.910152  [   90/  296]\n",
      "loss: 1.011875  [  130/  296]\n",
      "loss: 0.657756  [  170/  296]\n",
      "loss: 0.733696  [  210/  296]\n",
      "loss: 1.078966  [  250/  296]\n",
      "loss: 0.960412  [  290/  296]\n",
      "training loss: 24.04438\n",
      "Correct/Total: 68/129\n",
      "Validation Accuracy: 52.71318%\n",
      "\n",
      "Current Best Validation Accuracy: 52.71318%\n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.537097  [   10/  296]\n",
      "loss: 0.670042  [   50/  296]\n",
      "loss: 0.585447  [   90/  296]\n",
      "loss: 0.927452  [  130/  296]\n",
      "loss: 1.087495  [  170/  296]\n",
      "loss: 0.583157  [  210/  296]\n",
      "loss: 0.936245  [  250/  296]\n",
      "loss: 0.861394  [  290/  296]\n",
      "training loss: 23.19323\n",
      "Correct/Total: 41/129\n",
      "Validation Accuracy: 31.78295%\n",
      "\n",
      "Current Best Validation Accuracy: 52.71318%\n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.783304  [   10/  296]\n",
      "loss: 0.840553  [   50/  296]\n",
      "loss: 0.461200  [   90/  296]\n",
      "loss: 0.420594  [  130/  296]\n",
      "loss: 0.677152  [  170/  296]\n",
      "loss: 0.292765  [  210/  296]\n",
      "loss: 0.818273  [  250/  296]\n",
      "loss: 0.977365  [  290/  296]\n",
      "training loss: 21.18810\n",
      "Correct/Total: 69/129\n",
      "Validation Accuracy: 53.48837%\n",
      "\n",
      "Current Best Validation Accuracy: 53.48837%\n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.476444  [   10/  296]\n",
      "loss: 0.599916  [   50/  296]\n",
      "loss: 0.694667  [   90/  296]\n",
      "loss: 0.535903  [  130/  296]\n",
      "loss: 0.374229  [  170/  296]\n",
      "loss: 0.645507  [  210/  296]\n",
      "loss: 0.981088  [  250/  296]\n",
      "loss: 1.176156  [  290/  296]\n",
      "training loss: 23.34446\n",
      "Correct/Total: 69/129\n",
      "Validation Accuracy: 53.48837%\n",
      "\n",
      "Current Best Validation Accuracy: 53.48837%\n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.360245  [   10/  296]\n",
      "loss: 1.106738  [   50/  296]\n",
      "loss: 0.750517  [   90/  296]\n",
      "loss: 0.805017  [  130/  296]\n",
      "loss: 0.592491  [  170/  296]\n",
      "loss: 0.925935  [  210/  296]\n",
      "loss: 0.910775  [  250/  296]\n",
      "loss: 1.290968  [  290/  296]\n",
      "training loss: 26.00517\n",
      "Correct/Total: 66/129\n",
      "Validation Accuracy: 51.16279%\n",
      "\n",
      "Current Best Validation Accuracy: 53.48837%\n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 1.044879  [   10/  296]\n",
      "loss: 0.329001  [   50/  296]\n",
      "loss: 0.753607  [   90/  296]\n",
      "loss: 0.654577  [  130/  296]\n",
      "loss: 0.883050  [  170/  296]\n",
      "loss: 1.062790  [  210/  296]\n",
      "loss: 0.759965  [  250/  296]\n",
      "loss: 0.782729  [  290/  296]\n",
      "training loss: 24.73861\n",
      "Correct/Total: 52/129\n",
      "Validation Accuracy: 40.31008%\n",
      "\n",
      "Current Best Validation Accuracy: 53.48837%\n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.725085  [   10/  296]\n",
      "loss: 0.672257  [   50/  296]\n",
      "loss: 0.730194  [   90/  296]\n",
      "loss: 0.784139  [  130/  296]\n",
      "loss: 0.753279  [  170/  296]\n",
      "loss: 0.535450  [  210/  296]\n",
      "loss: 0.970467  [  250/  296]\n",
      "loss: 0.648520  [  290/  296]\n",
      "training loss: 22.54858\n",
      "Correct/Total: 73/129\n",
      "Validation Accuracy: 56.58915%\n",
      "\n",
      "Current Best Validation Accuracy: 56.58915%\n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.958824  [   10/  296]\n",
      "loss: 0.813094  [   50/  296]\n",
      "loss: 0.946993  [   90/  296]\n",
      "loss: 0.930475  [  130/  296]\n",
      "loss: 0.991647  [  170/  296]\n",
      "loss: 0.460798  [  210/  296]\n",
      "loss: 1.199527  [  250/  296]\n",
      "loss: 0.856936  [  290/  296]\n",
      "training loss: 22.78728\n",
      "Correct/Total: 70/129\n",
      "Validation Accuracy: 54.26357%\n",
      "\n",
      "Current Best Validation Accuracy: 56.58915%\n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.874941  [   10/  296]\n",
      "loss: 0.528886  [   50/  296]\n",
      "loss: 1.019438  [   90/  296]\n",
      "loss: 0.596487  [  130/  296]\n",
      "loss: 0.444820  [  170/  296]\n",
      "loss: 0.740564  [  210/  296]\n",
      "loss: 0.517421  [  250/  296]\n",
      "loss: 0.511358  [  290/  296]\n",
      "training loss: 19.74614\n",
      "Correct/Total: 68/129\n",
      "Validation Accuracy: 52.71318%\n",
      "\n",
      "Current Best Validation Accuracy: 56.58915%\n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.755442  [   10/  296]\n",
      "loss: 0.742200  [   50/  296]\n",
      "loss: 0.701915  [   90/  296]\n",
      "loss: 0.874813  [  130/  296]\n",
      "loss: 0.646835  [  170/  296]\n",
      "loss: 0.988041  [  210/  296]\n",
      "loss: 0.995588  [  250/  296]\n",
      "loss: 0.905297  [  290/  296]\n",
      "training loss: 20.67575\n",
      "Correct/Total: 71/129\n",
      "Validation Accuracy: 55.03876%\n",
      "\n",
      "Current Best Validation Accuracy: 56.58915%\n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.594384  [   10/  296]\n",
      "loss: 1.168330  [   50/  296]\n",
      "loss: 0.540773  [   90/  296]\n",
      "loss: 0.710532  [  130/  296]\n",
      "loss: 0.500905  [  170/  296]\n",
      "loss: 0.771889  [  210/  296]\n",
      "loss: 0.930228  [  250/  296]\n",
      "loss: 0.356516  [  290/  296]\n",
      "training loss: 22.36524\n",
      "Correct/Total: 67/129\n",
      "Validation Accuracy: 51.93798%\n",
      "\n",
      "Current Best Validation Accuracy: 56.58915%\n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.494508  [   10/  296]\n",
      "loss: 0.991548  [   50/  296]\n",
      "loss: 0.585165  [   90/  296]\n",
      "loss: 0.697768  [  130/  296]\n",
      "loss: 1.211909  [  170/  296]\n",
      "loss: 0.769106  [  210/  296]\n",
      "loss: 0.513725  [  250/  296]\n",
      "loss: 0.704133  [  290/  296]\n",
      "training loss: 21.40789\n",
      "Correct/Total: 73/129\n",
      "Validation Accuracy: 56.58915%\n",
      "\n",
      "Current Best Validation Accuracy: 56.58915%\n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.776918  [   10/  296]\n",
      "loss: 0.523629  [   50/  296]\n",
      "loss: 0.967462  [   90/  296]\n",
      "loss: 0.903516  [  130/  296]\n",
      "loss: 0.791574  [  170/  296]\n",
      "loss: 0.653307  [  210/  296]\n",
      "loss: 0.801459  [  250/  296]\n",
      "loss: 0.827151  [  290/  296]\n",
      "training loss: 23.49693\n",
      "Correct/Total: 71/129\n",
      "Validation Accuracy: 55.03876%\n",
      "\n",
      "Current Best Validation Accuracy: 56.58915%\n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 1.139216  [   10/  296]\n",
      "loss: 0.598633  [   50/  296]\n",
      "loss: 0.704302  [   90/  296]\n",
      "loss: 1.101709  [  130/  296]\n",
      "loss: 0.577936  [  170/  296]\n",
      "loss: 0.878174  [  210/  296]\n",
      "loss: 0.831556  [  250/  296]\n",
      "loss: 0.849915  [  290/  296]\n",
      "training loss: 21.65795\n",
      "Correct/Total: 70/129\n",
      "Validation Accuracy: 54.26357%\n",
      "\n",
      "Current Best Validation Accuracy: 56.58915%\n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.963334  [   10/  296]\n",
      "loss: 0.497104  [   50/  296]\n",
      "loss: 0.738439  [   90/  296]\n",
      "loss: 0.850089  [  130/  296]\n",
      "loss: 0.920146  [  170/  296]\n",
      "loss: 0.852199  [  210/  296]\n",
      "loss: 0.549899  [  250/  296]\n",
      "loss: 0.607197  [  290/  296]\n",
      "training loss: 21.88463\n",
      "Correct/Total: 71/129\n",
      "Validation Accuracy: 55.03876%\n",
      "\n",
      "Current Best Validation Accuracy: 56.58915%\n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.895331  [   10/  296]\n",
      "loss: 1.272987  [   50/  296]\n",
      "loss: 1.139733  [   90/  296]\n",
      "loss: 0.296675  [  130/  296]\n",
      "loss: 0.799216  [  170/  296]\n",
      "loss: 0.877040  [  210/  296]\n",
      "loss: 0.878561  [  250/  296]\n",
      "loss: 0.606672  [  290/  296]\n",
      "training loss: 23.79006\n",
      "Correct/Total: 72/129\n",
      "Validation Accuracy: 55.81396%\n",
      "\n",
      "Current Best Validation Accuracy: 56.58915%\n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.783400  [   10/  296]\n",
      "loss: 0.618823  [   50/  296]\n",
      "loss: 0.722849  [   90/  296]\n",
      "loss: 0.490008  [  130/  296]\n",
      "loss: 1.146441  [  170/  296]\n",
      "loss: 0.819564  [  210/  296]\n",
      "loss: 0.539929  [  250/  296]\n",
      "loss: 1.039290  [  290/  296]\n",
      "training loss: 21.88103\n",
      "Correct/Total: 71/129\n",
      "Validation Accuracy: 55.03876%\n",
      "\n",
      "Current Best Validation Accuracy: 56.58915%\n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 1.265570  [   10/  296]\n",
      "loss: 0.933099  [   50/  296]\n",
      "loss: 0.712698  [   90/  296]\n",
      "loss: 0.813058  [  130/  296]\n",
      "loss: 0.650850  [  170/  296]\n",
      "loss: 0.611755  [  210/  296]\n",
      "loss: 0.918217  [  250/  296]\n",
      "loss: 0.514205  [  290/  296]\n",
      "training loss: 22.99005\n",
      "Correct/Total: 71/129\n",
      "Validation Accuracy: 55.03876%\n",
      "\n",
      "Current Best Validation Accuracy: 56.58915%\n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.472234  [   10/  296]\n",
      "loss: 0.472378  [   50/  296]\n",
      "loss: 0.403465  [   90/  296]\n",
      "loss: 0.713427  [  130/  296]\n",
      "loss: 0.647085  [  170/  296]\n",
      "loss: 0.601309  [  210/  296]\n",
      "loss: 0.599078  [  250/  296]\n",
      "loss: 0.670262  [  290/  296]\n",
      "training loss: 21.37550\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(training_data_loader, model, loss_function, optimizer)\n",
    "    val_loss = validation(validation_data, model, loss_function)\n",
    "    scheduler.step(val_loss)\n",
    "    # torch.save(model, 'model.pth')\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WjAM8Xqv0IG7",
   "metadata": {
    "id": "WjAM8Xqv0IG7"
   },
   "outputs": [],
   "source": [
    "validation(validation_data, model, loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DXTLjNfmAQ87",
   "metadata": {
    "id": "DXTLjNfmAQ87"
   },
   "outputs": [],
   "source": [
    "plt.plot(validation_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aAVt6hCTakad",
   "metadata": {
    "id": "aAVt6hCTakad"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
